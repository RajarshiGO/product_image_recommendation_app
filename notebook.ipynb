{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image similarity based recommendation system for E-commerce platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project has been implemented on image dataset provided by [Amazon-Berkeley](https://amazon-berkeley-objects.s3.amazonaws.com/index.html), specifically the small version of the dataset. So, the first step will be to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import albumentations as albu\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing with Spark\n",
    "The dataset consists of some images with a very small aspect ratio. These essentially text labels of the products and practically useless for our application. Therefore, we will be removing them using spark because spark provides a convenient way to filter out such images quickly and without writing much code, so initialize a spark session in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 18:20:59 WARN Utils: Your hostname, starfire resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface enp4s0)\n",
      "22/10/07 18:20:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 18:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the images in the nested directory into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 18:00:42 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    }
   ],
   "source": [
    "image_df = spark.read.format(\"image\").load(\"./small/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+-----+------+\n",
      "|origin                                                                         |width|height|\n",
      "+-------------------------------------------------------------------------------+-----+------+\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/f5/f570c185.jpg|244  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/dc/dc7e130d.png|256  |144   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/81/81614547.jpg|241  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/51/51b6b436.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/09/098bc917.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/a4/a42faf60.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/e2/e21f8a61.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/d0/d0a22d53.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/39/39ffbe14.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/5b/5b476e6d.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/a6/a64279e4.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/54/54faa5bd.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/01/01e5740b.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/65/6584cde3.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/8b/8b515066.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/17/171f6058.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/bf/bf48ca25.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/ac/ac14e1a9.jpg|256  |256   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/5b/5bb1d7e3.jpg|256  |185   |\n",
      "|file:/home/raj/Projects/Image_RecommendationSystem/images/small/ae/ae923901.jpg|256  |256   |\n",
      "+-------------------------------------------------------------------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "image_df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we filter images that have a difference of more than 200 between their width and height and then extract their paths into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "paths = image_df.filter((abs(image_df.image.width - image_df.image.height))>=200).select(\"image.origin\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the filtered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    os.remove(path.lstrip('file:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pre-trained VGG16 model from keras and attach a convolution layer over it with 128 filters to reduce the output features. This will act as our feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = keras.applications.VGG16(input_shape = (256, 256, 3), include_top = False, weights = 'imagenet')\n",
    "x = keras.layers.Conv2D(128, 5, activation='relu')(vgg.output)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "model = keras.Model(inputs=vgg.input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our method is an unsupervised one, it is likely that it will be biased with objects of the same colour and there is also a possibility that the object of interest might be in a cluttered environment, so we need to apply a pre-processing to the images that enhances the contrast and then converts it to gray scale. For this we will use the albumentations library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = albu.Compose([\n",
    "    albu.CLAHE(p=1),\n",
    "    albu.ToGray(p=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the image paths in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(\"./small/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a very large number of images, it will be wise to make use of data generator for the feature extraction. Let's define one first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, image_paths, batch_size):\n",
    "        'Initialization'\n",
    "        self.image_paths = image_paths\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        paths = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(paths)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __data_generation(self, paths):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, 256, 256, 3))\n",
    "        y = list()\n",
    "\n",
    "        # Generate data\n",
    "        for i, path in enumerate(paths):\n",
    "            image = Image.open(path)\n",
    "            image = image.convert('RGB')\n",
    "            image = image.resize((256, 256))\n",
    "            image = np.array(image)\n",
    "            image = preprocess(image = image)\n",
    "            X[i,] = image['image']/255\n",
    "            y.append(path)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the data generator with a batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator(image_paths, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from image batches and save them in a file along with their respective file paths in two separate folders: features and labels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (im, p) in enumerate(tqdm(generator)):\n",
    "    with open(\"./features/batch{}.npy\".format(i), \"wb\") as f:\n",
    "        np.save(f, model(im))\n",
    "    with open(\"./labels/batch{}.npy\".format(i), \"wb\") as g:\n",
    "        np.save(g, np.array(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Approximate Nearest Neighbor search model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the massive size of the database, the conventional nearest neighbor search algorithm will be very slow. To mitigate this, we have used approximate nearest neighbor search algorithm implemented in the Spotify/Annoy library. Therefore, get the features from each of the files generated earlier and then feed them to an annoy index instance initialized with a feature length of 128 after normalizing each feature vector. Also, generate the list of image files corresponding to each feature index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = os.listdir(\"./features\")\n",
    "t = AnnoyIndex(128, 'euclidean')\n",
    "files_list = list()\n",
    "c = 0\n",
    "for file in feature_files:\n",
    "    fea = np.load(os.path.join(\"./features\", file))\n",
    "    file_names = np.load(os.path.join(\"./labels\", file))\n",
    "    for i in range(fea.shape[0]):\n",
    "        t.add_item(c, fea[i]/np.linalg.norm(fea[i]))\n",
    "        c += 1\n",
    "        files_list.append(file_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the annoy index with 10 trees and save it to file which can be used to design an actual application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.build(10)\n",
    "t.save(\"nearest_neighbor.ann\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the feature extraction deep learning model along with file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')\n",
    "file_list = open(\"files.pkl\", \"wb\")\n",
    "pkl.dump(files_list, file_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
